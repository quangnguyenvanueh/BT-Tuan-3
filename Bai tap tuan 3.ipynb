{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header = 0, delimiter = '\\t')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 287032),\n",
       " ('a', 155096),\n",
       " ('and', 152664),\n",
       " ('of', 142972),\n",
       " ('to', 132568),\n",
       " ('is', 103228),\n",
       " ('in', 85580),\n",
       " ('I', 65973),\n",
       " ('that', 64560),\n",
       " ('this', 57196),\n",
       " ('it', 54429),\n",
       " ('/><br', 50935),\n",
       " ('was', 46698),\n",
       " ('as', 42510),\n",
       " ('with', 41721),\n",
       " ('for', 41070),\n",
       " ('but', 33790),\n",
       " ('The', 33762),\n",
       " ('on', 30766),\n",
       " ('movie', 30500)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "for twit in train['review']:\n",
    "    for word in twit.split():\n",
    "        vocab[word] += 1\n",
    "vocab.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Asus\n",
      "[nltk_data]     K43E\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('/><br', 50935),\n",
       " ('movie', 30500),\n",
       " ('film', 27397),\n",
       " ('one', 20688),\n",
       " ('like', 18133),\n",
       " ('would', 11922),\n",
       " ('good', 11435),\n",
       " ('really', 10815),\n",
       " ('even', 10607),\n",
       " ('see', 10155),\n",
       " ('-', 9355),\n",
       " ('get', 8777),\n",
       " ('story', 8526),\n",
       " ('much', 8507),\n",
       " ('time', 7764),\n",
       " ('make', 7485),\n",
       " ('could', 7462),\n",
       " ('also', 7422),\n",
       " ('first', 7339),\n",
       " ('people', 7335)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "print(stop)\n",
    "vocab_reduced = Counter()\n",
    "for w, c in vocab.items():\n",
    "    if not w.lower() in stop:\n",
    "        vocab_reduced[w] = c\n",
    "\n",
    "vocab_reduced.most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like it  :)\n"
     ]
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "print(preprocessor('I like it --- :.; :))))'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'there,', 'I', 'am', 'loving', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n",
      "['Hi', 'there,', 'I', 'am', 'love', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "print(tokenizer('Hi there, I am loving this, like with a lot of love'))\n",
    "print(tokenizer_porter('Hi there, I am loving this, like with a lot of love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train['review']\n",
    "y = train['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function preproc...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)\n",
    "\n",
    "# A pipeline is what chains several steps together, once the initial exploration is done. \n",
    "# For example, some codes are meant to transform features — normalise numericals, or turn text into vectors, \n",
    "# or fill up missing data, they are transformers; other codes are meant to predict variables by fitting an algorithm,\n",
    "# they are estimators. Pipeline chains all these together which can then be applied to training data\n",
    "clf = Pipeline([('vect', tfidf),\n",
    "                ('clf', LogisticRegression(random_state=0))])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8853333333333333\n",
      "confusion matrix:\n",
      " [[3293  503]\n",
      " [ 357 3347]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.87      0.88      3796\n",
      "          1       0.87      0.90      0.89      3704\n",
      "\n",
      "avg / total       0.89      0.89      0.89      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "predictions = clf.predict(X_test)\n",
    "print('accuracy:',accuracy_score(y_test,predictions))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is really bad --> Negative, Positive = [1. 0.]\n",
      "I love this! --> Negative, Positive = [0.02 0.98]\n",
      ":) --> Negative, Positive = [0.19 0.81]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9094</th>\n",
       "      <td>8728_7</td>\n",
       "      <td>1</td>\n",
       "      <td>Pixar has had massive success over the years w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11022</th>\n",
       "      <td>477_10</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi all I am a chess enthusiast since the age o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9346</th>\n",
       "      <td>11130_9</td>\n",
       "      <td>1</td>\n",
       "      <td>Jon Voight is brilliant in Midnight Cowboy, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6808</th>\n",
       "      <td>2240_7</td>\n",
       "      <td>1</td>\n",
       "      <td>My abiding love of Italian actress Lucianna Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19163</th>\n",
       "      <td>8610_2</td>\n",
       "      <td>0</td>\n",
       "      <td>As much as I love Rodney Dangerfield, this was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>6255_8</td>\n",
       "      <td>1</td>\n",
       "      <td>This film show peoples in the middle of the ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>3820_8</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a story about Shin-ae, who moves to Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14160</th>\n",
       "      <td>6172_7</td>\n",
       "      <td>1</td>\n",
       "      <td>It is wonderful to watch Roshan Seth (the stri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8082</th>\n",
       "      <td>5020_3</td>\n",
       "      <td>0</td>\n",
       "      <td>This is the most messed up entry on IMDb that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18937</th>\n",
       "      <td>5326_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\\Birth of the Beatles\\\", for being a US televi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sentiment                                             review\n",
       "9094    8728_7          1  Pixar has had massive success over the years w...\n",
       "11022   477_10          1  Hi all I am a chess enthusiast since the age o...\n",
       "9346   11130_9          1  Jon Voight is brilliant in Midnight Cowboy, bu...\n",
       "6808    2240_7          1  My abiding love of Italian actress Lucianna Pa...\n",
       "19163   8610_2          0  As much as I love Rodney Dangerfield, this was...\n",
       "1056    6255_8          1  This film show peoples in the middle of the ho...\n",
       "844     3820_8          1  This is a story about Shin-ae, who moves to Mi...\n",
       "14160   6172_7          1  It is wonderful to watch Roshan Seth (the stri...\n",
       "8082    5020_3          0  This is the most messed up entry on IMDb that ...\n",
       "18937  5326_10          1  \\Birth of the Beatles\\\", for being a US televi..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twits = [\n",
    "    \"This is really bad\",\n",
    "    \"I love this!\",\n",
    "    \":)\",\n",
    "]\n",
    "\n",
    "preds = clf.predict_proba(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')\n",
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\logisticRegression.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-128feade3217>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logisticRegression.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\logisticRegression.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "pickle.dump(clf, open(os.path.join('data', 'logisticRegression.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
